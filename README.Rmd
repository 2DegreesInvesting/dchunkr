---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# dchunkr

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
[![R-CMD-check](https://github.com/2DegreesInvesting/dchunkr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/2DegreesInvesting/dchunkr/actions/workflows/R-CMD-check.yaml)
<!-- badges: end -->

The goal of dchunkr is provide basic tools for computing on data bigger than you
can fit in memory. Use it to chunks your data, to run multiple chunks in
parallel, and to cache the result of each chunk. This allows you to use your
computer resources more efficiently, and to resume after interruptions. It's
designed to be a minimal and to play well with the tidyverse and friends. For a
mature, complete alternative see the wonderful
[targets](https://docs.ropensci.org/targets/) package.

## Installation

You can install the development version of dchunkr from [GitHub](https://github.com/) with:

``` r
# install.packages("pak")
pak::pak("2DegreesInvesting/dchunkr")
```

## Example

```{r example}
library(dplyr, warn.conflicts = FALSE)
library(readr)
library(future)
library(furrr)
library(fs)
library(dchunkr)

# Enable computing over multiple workers in parallel
plan(multisession)

data <- tibble(id = c(1, 1, 1, 2, 3))

job <- data |>
  # Each chunk can run parallel to other chunks
  nest_chunk(.by = "id", chunks = 3) |>
  # Set where to cache the result of each chunk
  add_file(parent = cache_path("demo"), ext = ".csv") |>
  # Don't recompute what's already cached, so you can resume after interruptions
  pick_undone()
job

# Here is the important function you want to run for each chunk of data
important <- function(data) mutate(data, x2 = id * 2)

job |>
  # Select the columns that match the signature of the function passed to pmap
  select(data, file) |>
  # Map your important fuction to each chunk and write the result to the cache
  future_pwalk(\(data, file) important(data) |> write_csv(file))

# See cached files
dir_tree(cache_path("demo"))

# Read all cached files at once
read_csv(dir_ls(cache_path("demo")))

# Cleanup before the next run
cache_path("demo") |> dir_delete()
```
