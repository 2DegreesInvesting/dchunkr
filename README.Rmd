---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# dchunkr

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
[![R-CMD-check](https://github.com/2DegreesInvesting/dchunkr/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/2DegreesInvesting/dchunkr/actions/workflows/R-CMD-check.yaml)
<!-- badges: end -->

The goal of dchunkr is to help you to work with chunks of data in parallel and
to cache the results of each chunk. For a more complete approach see the
[targets](https://docs.ropensci.org/targets/) package.

## Installation

You can install the development version of dchunkr from [GitHub](https://github.com/) with:

``` r
# install.packages("pak")
pak::pak("2DegreesInvesting/dchunkr")
```

## Example

```{r example}
library(dplyr, warn.conflicts = FALSE)
library(readr)
library(future)
library(furrr)
library(purrr)
library(fs)
library(dchunkr)

# Enable computing over multiple workers in parallel
plan(multisession)

data <- tibble(id = c(1, 1, 1, 2, 3))

job <- data |> 
  # Each chunk can run parallel to other chunks
  nest_chunk(.by = "id", chunks = 3) |> 
  # Set where to cache the result of each chunk
  add_file(parent = cache_path("demo"), ext = ".csv") |> 
  # Don't recompute what's already cached, so you can resume after interruptions
  pick_undone()
job

job |> 
  # Select the columns that match the signature of the function passed to pmap
  select(data, file) |> 
  future_pwalk(\(data, file) mutate(data, x2 = id * 2) |> write_csv(file))

# See cached files
dir_tree(cache_path("demo"))

# Read all cached files at once
read_csv(dir_ls(cache_path("demo")))

# Cleanup before the next run
cache_path("demo") |> dir_delete()
```





